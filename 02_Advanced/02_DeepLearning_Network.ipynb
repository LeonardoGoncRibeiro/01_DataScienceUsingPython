{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_DeepLearning_Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN5zAxYJ6iNXY6AXZbf9ZM7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardoGoncRibeiro/01_DataScienceUsingPython/blob/main/02_Advanced/02_DeepLearning_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: How does the network learns?\n",
        "\n",
        "In this course, we will go further into the specificities of a neural network. We will understand how it works, and how does it learn. This will give us a better sense on:\n",
        "* Why these models are so easily generalized.\n",
        "* Why, sometimes, it is so hard to fit a model.\n",
        "* Understand some very important parameters to prevent overfitting.\n",
        "* Why model building may take a long time in some cases.\n",
        "\n",
        "In this course, we will use the following packages:"
      ],
      "metadata": {
        "id": "qVUUqkh-lwfH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3xqUFD_lskp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, similar to the previous course, we will use the Fashion MNIST dataset:"
      ],
      "metadata": {
        "id": "yUCgRt8Pm8Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "((X_train, y_train), (X_test, y_test)) = keras.datasets.fashion_mnist.load_data( )"
      ],
      "metadata": {
        "id": "0bUxTaT_m6Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding a base model\n",
        "\n",
        "Now, let's fit a basic model:"
      ],
      "metadata": {
        "id": "HxvCp1HQniVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "model = keras.Sequential([\n",
        "                          keras.layers.Flatten(input_shape = (28, 28)),\n",
        "                          keras.layers.Dense(256, activation = tf.nn.relu),\n",
        "                          keras.layers.Dropout(0.2),\n",
        "                          keras.layers.Dense(10,  activation = tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "fit_history = model.fit(X_train, y_train, epochs = 5, validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vy_2xennEQp",
        "outputId": "ca463b70-c65d-47e1-a8f3-926bf9dd1390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 3.0205 - accuracy: 0.6421 - val_loss: 0.8112 - val_accuracy: 0.7190\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8020 - accuracy: 0.7110 - val_loss: 0.6408 - val_accuracy: 0.7702\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.7415 - accuracy: 0.7298 - val_loss: 0.6127 - val_accuracy: 0.7918\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.7179 - accuracy: 0.7406 - val_loss: 0.5911 - val_accuracy: 0.7828\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.7294 - accuracy: 0.7366 - val_loss: 0.5934 - val_accuracy: 0.8072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have fitted a base model. Note that our training accuracy is 74.29%, and our validation accuracy is 82.52%. Let's check the test accuracy:"
      ],
      "metadata": {
        "id": "p-Bem7OqoaM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRFSdodFoBPY",
        "outputId": "ab871d8f-9f74-49ff-c42f-5167454fdbce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6126 - accuracy: 0.7970\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6125737428665161, 0.796999990940094]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very close to the validation accuracy. \n",
        "\n",
        "Ok, but how is our model working internally? Let's check a summary for our model:"
      ],
      "metadata": {
        "id": "zbzZ26guqLAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary( )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH6uCm67qJ8j",
        "outputId": "4138ab2c-72fc-41a3-95c8-d0c5f10d808b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 256)               200960    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 203,530\n",
            "Trainable params: 203,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have four layers: a flatten layer, a dense layer, a dropout layer, and a final dense layer. Then flatten layer has 784 neurons, the dense and dropout layers have 256 neurons, and the final dense layer has 10 neurons. \n",
        "\n",
        "Note that only the dense layers have parameters. The first dense layer has 200960 parameters. This happens because the Relu activation function has a weight parameter. Since the dense layer is connecting every neuron from the previous layer (784) to every neuron from the current layer (256), we end up with 200704 weight vectors. The rest of the hyperparameters are the biases given at each neuron. Thus, $200704 + 256 = 200960$. We can get the parameters of our model using keras. \n",
        "\n",
        "When we are fitting our neural network, we are effectively fitting those weights and biases. Initially, our model sets a random value for each parameter and, throughout the optimization process, the model adjusts those parameters until an optimal configuration is found. This optimal configuration represents the set of hyperparameters that optimizes a given metric (e.g. minimizes a loss function).\n",
        "\n",
        "To understand the configuration of our neural network, we can use:"
      ],
      "metadata": {
        "id": "qfICq40jqUqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_config( )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90JVL-io9ikg",
        "outputId": "0c69253c-8bb4-43cc-d4fa-30962f7e1047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layers': [{'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 28, 28),\n",
              "    'dtype': 'float32',\n",
              "    'name': 'flatten_4_input',\n",
              "    'ragged': False,\n",
              "    'sparse': False}},\n",
              "  {'class_name': 'Flatten',\n",
              "   'config': {'batch_input_shape': (None, 28, 28),\n",
              "    'data_format': 'channels_last',\n",
              "    'dtype': 'float32',\n",
              "    'name': 'flatten_4',\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_8',\n",
              "    'trainable': True,\n",
              "    'units': 256,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dropout',\n",
              "   'config': {'dtype': 'float32',\n",
              "    'name': 'dropout_4',\n",
              "    'noise_shape': None,\n",
              "    'rate': 0.2,\n",
              "    'seed': None,\n",
              "    'trainable': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'softmax',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_9',\n",
              "    'trainable': True,\n",
              "    'units': 10,\n",
              "    'use_bias': True}}],\n",
              " 'name': 'sequential_4'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we have a JSON containing a set of parameters for each layer. The initialization of the weights is performed using the Glorot Uniform algorithm. The biases, however, are being initialized with zeros."
      ],
      "metadata": {
        "id": "JIueMb5mAxlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relu activation function\n",
        "\n",
        "The Relu activation is given by:\n",
        "\n",
        "\\begin{equation}\n",
        "i(x) = \\max(0, x)\n",
        "\\end{equation}\n",
        "\n",
        "Thus, if the value is negative, the Relu returns 0. Else, it returns the value itself. The output for the neuron, however, gets the Relu function, multiplies by the weight for the connection, and adds the bias for the neuron. Thus, we can say that:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Output} = i*w + b\n",
        "\\end{equation}\n",
        "\n",
        "Since we are dealing with multiple neurons, we can write this function as the multiplication between two matrices, and then the sum with another matrix. Thus, essentially, what happens which deep learning is a multiplication and sum of matrices. "
      ],
      "metadata": {
        "id": "4Rqd6gDJPd7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network optimization\n",
        "\n",
        "Thus, our Neural Network is enterily dependent on the weights and bias. To get the optimal weight and bias, our model uses the gradient descent algorithm. \n",
        "\n",
        "The gradient descent algorithm is an iterative algorithm which continuously changes the values of our parameters until it reachs the optimum. The change in the value of the weights for each iteration is called momentum. If we have a high momentum, we will not be able to find to optimal point, since our algorithm will not be able to stop in the exact optimal. If we have a low momentum, we will likely find it, but it may take very long. \n",
        "\n",
        "Also, note that the gradient descent algorithm only guarantees that we find the optimum if we are optimizing a simple unimodal function, with only one local minima (which is the same as the global minima). If we have a multimodal function, we may end up finding a local minima which is different from the true global.\n",
        "\n",
        "Some things can help the algorithm to avoid those local minima. For instance, using Dropout layers can help us to continue exploring new possible sets of parameters even if we found a local minima. Also, the use of an stochastic gradient descent algorithm also makes it so that our algorithm does not stop upon reaching a local minima. \n",
        "\n",
        "Note that we are using the Adam algorithm as the optimizer. Adam is a very good optimizer, where the moment is adaptively estimated by the algorithm. Also, it uses the stochastic gradient descent to find the optimum. To understand if our model has converged, we may look for the decrease in loss for each epoch. If it has stabilized, it means that our model converged, and likely will not be able to be improved (unless we change the model parameters).\n",
        "\n",
        "The Adam optimizer has some parameters, which may assist in the optimization process. To get information about the Adam optimizer, one may look for:\n",
        "\n",
        "https://keras.io/api/optimizers/adam/\n",
        "\n",
        "For instance, to change the momentum of our network, we can use the learning rate parameter (```lr```). Thus, we may do:"
      ],
      "metadata": {
        "id": "sjQZdZvZQ5iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "model = keras.Sequential([\n",
        "                          keras.layers.Flatten(input_shape = (28, 28)),\n",
        "                          keras.layers.Dense(256, activation = tf.nn.relu),\n",
        "                          keras.layers.Dropout(0.2),\n",
        "                          keras.layers.Dense(10,  activation = tf.nn.softmax)\n",
        "])\n",
        "\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.1)\n",
        "\n",
        "model.compile(optimizer = adam, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "fit_history = model.fit(X_train, y_train, epochs = 5, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "41f8t8xdAv8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59317ff-f217-4799-e67a-e2d62d8648d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 77.2873 - accuracy: 0.1034 - val_loss: 2.4321 - val_accuracy: 0.1027\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 2.3158 - accuracy: 0.1012 - val_loss: 2.4288 - val_accuracy: 0.1027\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 2.3190 - accuracy: 0.0989 - val_loss: 2.4325 - val_accuracy: 0.1013\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 2.3160 - accuracy: 0.0993 - val_loss: 2.4349 - val_accuracy: 0.0957\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 2.3168 - accuracy: 0.0985 - val_loss: 2.4351 - val_accuracy: 0.1013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, by default, the learning rate for Adam is 0.001. Thus, here, we are increasing the learning rate of our optimizer. When we use a very high learning rate, our model is not able to find the optimal value. That occurs because our momentum is too high, and the optimizer is not able to find the optimum. Our accuracy is very close to 10%, which is very close to the accuracy we would expect from a dummy optimizer (since we have 10 possible labels). \n",
        "\n",
        "Also, we can make keras identify when the model has converged (or is not being able to improve upon the current optimum), so that it may stop prematurely. For that end, we can do:"
      ],
      "metadata": {
        "id": "1ePA_P-lnk3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "model = keras.Sequential([\n",
        "                          keras.layers.Flatten(input_shape = (28, 28)),\n",
        "                          keras.layers.Dense(256, activation = tf.nn.relu),\n",
        "                          keras.layers.Dropout(0.2),\n",
        "                          keras.layers.Dense(10,  activation = tf.nn.softmax)\n",
        "])\n",
        "\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.1)\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 1)]\n",
        "\n",
        "model.compile(optimizer = adam, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "fit_history = model.fit(X_train, y_train, epochs = 5, validation_split = 0.2, callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGSw3OF4uWip",
        "outputId": "08c78056-019e-4ff4-88f8-ad64d5739c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 77.2873 - accuracy: 0.1034 - val_loss: 2.4321 - val_accuracy: 0.1027\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 2.3158 - accuracy: 0.1012 - val_loss: 2.4288 - val_accuracy: 0.1027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! This time, instead of performing 5 epochs, we only perform 2, and the model itself understood that the model would not improve any further. For that, it looked for the validation accuracy."
      ],
      "metadata": {
        "id": "sGa_q5Cvvp9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's try to use a lower learning rate, but slightly higher learning rate than the default value:"
      ],
      "metadata": {
        "id": "X-pQffnruF6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "model = keras.Sequential([\n",
        "                          keras.layers.Flatten(input_shape = (28, 28)),\n",
        "                          keras.layers.Dense(256, activation = tf.nn.relu),\n",
        "                          keras.layers.Dropout(0.2),\n",
        "                          keras.layers.Dense(10,  activation = tf.nn.softmax)\n",
        "])\n",
        "\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.002)\n",
        "\n",
        "model.compile(optimizer = adam, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "fit_history = model.fit(X_train, y_train, epochs = 5, validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X1tXmcLopT6",
        "outputId": "fe93ce00-fae9-4709-ed4b-ae6d9c7fa1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 7s 4ms/step - loss: 3.4887 - accuracy: 0.5977 - val_loss: 0.8685 - val_accuracy: 0.7178\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.9982 - accuracy: 0.6526 - val_loss: 0.8051 - val_accuracy: 0.7212\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 1.0080 - accuracy: 0.6497 - val_loss: 0.8100 - val_accuracy: 0.7309\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 1.0277 - accuracy: 0.6425 - val_loss: 0.8778 - val_accuracy: 0.7360\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 1.1607 - accuracy: 0.5960 - val_loss: 0.8680 - val_accuracy: 0.7212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still, our accuracy reduced (comparing with our baseline). Thus, our former model, with ```lr = 0.001```, seems to be the best one for now.\n",
        "\n",
        "# Improving the efficiency of our model\n",
        "\n",
        "Another thing we can change here is to use only some data in each run of the optimizer. We can change this using the batch size parameter. Here, for instance, we will use ```batch_size = 480```, where 480 corresponds to 10% of our dataset."
      ],
      "metadata": {
        "id": "Qs1ZEyZnrRHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "model = keras.Sequential([\n",
        "                          keras.layers.Flatten(input_shape = (28, 28)),\n",
        "                          keras.layers.Dense(256, activation = tf.nn.relu),\n",
        "                          keras.layers.Dropout(0.2),\n",
        "                          keras.layers.Dense(10,  activation = tf.nn.softmax)\n",
        "])\n",
        "\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.002)\n",
        "\n",
        "model.compile(optimizer = adam, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "fit_history = model.fit(X_train, y_train, epochs = 5, batch_size = 480, validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8CQES3QqhlC",
        "outputId": "86b2998d-42bd-42af-faa5-0c9d500f64e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "100/100 [==============================] - 3s 20ms/step - loss: 22.1492 - accuracy: 0.5891 - val_loss: 1.0729 - val_accuracy: 0.6830\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 1.0700 - accuracy: 0.6660 - val_loss: 0.8658 - val_accuracy: 0.7209\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.9066 - accuracy: 0.7020 - val_loss: 0.7794 - val_accuracy: 0.7470\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.7841 - accuracy: 0.7304 - val_loss: 0.6726 - val_accuracy: 0.7810\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.7130 - accuracy: 0.7503 - val_loss: 0.6460 - val_accuracy: 0.7942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the change in accuracy was very minor (train accuracy was higher, and validation accuracy was lower). However, the time spent in each epoch was reduced a lot."
      ],
      "metadata": {
        "id": "S_7h4xSqsILK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model in its best state\n",
        "\n",
        "Note that, during our optimization process, our weights and biases are changed, and our model may, actually, get worse.\n",
        "\n",
        "We can try to save time by making a checkpoint of our model state, and save the model when it shows the highest validation accuracy. For that end, we may use:"
      ],
      "metadata": {
        "id": "EoJQUygnsuOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "del model\n",
        "\n",
        "model = keras.Sequential([\n",
        "                          keras.layers.Flatten(input_shape = (28, 28)),\n",
        "                          keras.layers.Dense(256, activation = tf.nn.relu),\n",
        "                          keras.layers.Dropout(0.2),\n",
        "                          keras.layers.Dense(10,  activation = tf.nn.softmax)\n",
        "])\n",
        "\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.001)\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(filepath = 'best_model.hdf5', monitor = 'val_loss', save_best_only = True)]\n",
        "\n",
        "model.compile(optimizer = adam, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "fit_history = model.fit(X_train, y_train, epochs = 5, validation_split = 0.2, batch_size = 480, callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQCZFn4Lr7d_",
        "outputId": "ad7adc7c-f9c8-46f4-866a-fb7b85e78c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 17.6192 - accuracy: 0.6351 - val_loss: 1.3690 - val_accuracy: 0.6959\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 1.3012 - accuracy: 0.6677 - val_loss: 0.9751 - val_accuracy: 0.7184\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 1.0088 - accuracy: 0.6924 - val_loss: 0.8364 - val_accuracy: 0.7425\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.8509 - accuracy: 0.7214 - val_loss: 0.7324 - val_accuracy: 0.7713\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.7610 - accuracy: 0.7458 - val_loss: 0.6764 - val_accuracy: 0.7857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Note that, here, the final model state was saved, as it presented the lower validation loss."
      ],
      "metadata": {
        "id": "nMTuSTgiyc3R"
      }
    }
  ]
}